# -*- coding: utf-8 -*-
"""PARSeq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TsR4eWDvzP_42QWntmy7OxeVbPuAYtA8

# **PARSeq: Train a Custom OCR Model**
The web content provides a detailed guide on training a custom OCR (Optical Character Recognition) model using PARSeq, particularly for non-English datasets, including steps for repository cloning, charset creation, dataset preparation, and training configuration.

# **1. Clone the repository**
"""

!git clone https://github.com/dilithjay/Sinhala-ParSeq.git

"""# **Install dependencies**"""

# Reinstall to resolve undefined character error with torchtext
!pip install torch==1.10.0 torchtext==0.11.0

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Sinhala-ParSeq/
!pip install -r requirements.txt
!pip install -e .
# Note: May have to restart runtime afterward



"""To create the dataset, the PARSeq repo provides an updated version of a python script that’s generally used for this purpose. Therefore, you can find the script here or inside the tools directory within the repo you cloned.

Before running the script, you will need the following:

A place to store the .lmdb file. One thing to note is that the drive you're saving to should have sufficient space. If you don't have about 1TB of free space in your drive. Change the map_size attribute of lmdb.open() from 1099511627776 to 1073741824.
A directory with all the training images. Make sure the names of the images do not have spaces.
A text file with all the image names and their respective text. So for example, the content of the text file may look something like this:

img_1.jpg hello

img_2.jpg world

img_3.jpg someword


# **Create the dataset yaml file**
Similar to charset, there’s also a dataset directory inside the configs directory. Accordingly, create a yaml file with the name of your dataset. You can simply duplicate one of the existing files and rename the yaml file's name and the train_dir attribute to the name of your dataset. The train_dir doesn't have to be the same name as your dataset, but it's easier this way. In my case, the name of my dataset is sin_hw, so the content of the sin_hw.yaml file is the following.

data: train_dir: sin_hw num_workers: 2

# **Train**

running the following command should start the training process:
"""

# Commented out IPython magic to ensure Python compatibility.


# %cd /content/Sinhala-ParSeq/
!python train.py

"""# **evaluation**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Sinhala-ParSeq/
from strhub.data.module import SceneTextDataModule

# Be sure to change the following as needed (or as specified in the config files)
root_dir = 'data'
train_dir = 'sin_printed'
batch_size = 8
img_size = [ 32, 128 ]
charset_train = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ01"
charset_test = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ01"
max_label_length = 25
augment = False
num_workers = 2

datamodule = SceneTextDataModule(root_dir, train_dir, img_size, max_label_length,
                 charset_train, charset_test, batch_size, num_workers, augment)


# %cd /content/Sinhala-ParSeq/
from strhub.models.utils import load_from_checkpoint
from glob import glob

# Load the latest model
checkpoints = glob('/content/Sinhala-ParSeq/outputs/parseq/*/checkpoints/last.ckpt')

checkpoint_path = sorted(checkpoints)[-1]
print(checkpoint_path)
model = load_from_checkpoint(checkpoint_path, charset_test=charset_test).eval().to('cuda')


from tqdm import tqdm

dataloader = datamodule.val_dataloader()

total = 0
correct = 0
ned = 0
confidence = 0
label_length = 0
for imgs, labels in tqdm(iter(dataloader)):
    res = model.test_step((imgs.to(model.device), labels), -1)['output']
    total += res.num_samples
    correct += res.correct
    ned += res.ned
    confidence += res.confidence
    label_length += res.label_length
accuracy = 100 * correct / total
mean_ned = 100 * (1 - ned / total)
mean_conf = 100 * confidence / total
mean_label_length = label_length / total

print(f"\nAccuracy: {accuracy}, Mean ned: {mean_ned}, Mean confidence: {mean_conf}, Mean label length: {mean_label_length}")


# Take a look at the images produced by the dataloader
import torchvision.transforms as T
transform = T.ToPILImage()
for images, labels in datamodule.val_dataloader():
  print(model.test_step((imgs.to(model.device), labels), -1)['output'])
  for i, img in enumerate(images):
    display(transform(img))
    print(labels[i])

  break





