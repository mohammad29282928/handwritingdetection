{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def read_images(input_folder):\n",
        "    \"\"\"\n",
        "    Reads all images from the input folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "\n",
        "    Returns:\n",
        "    List of image file paths.\n",
        "    \"\"\"\n",
        "    image_files = []\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "            image_files.append(os.path.join(input_folder, file))\n",
        "    return image_files\n",
        "\n",
        "def draw_boxes(image, boxes, color=(0, 255, 0)):\n",
        "    for box in boxes:\n",
        "        if len(box) == 4:  # Ensuring box has the correct format\n",
        "            cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n",
        "    return image\n",
        "\n",
        "def save_image(output_folder, filename, image):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    Image.fromarray(image).save(os.path.join(output_folder, filename + '_highlighted.png'))\n"
      ],
      "metadata": {
        "id": "hCbEPP5qTDGr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/In.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVtIh9PXUlhi",
        "outputId": "f4115453-2cf1-4d19-e509-a298e835d249"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/In.zip\n",
            "  inflating: In/1.jpg                \n",
            "  inflating: In/10.jpg               \n",
            "  inflating: In/11.jpg               \n",
            "  inflating: In/12.png               \n",
            "  inflating: In/2.jpg                \n",
            "  inflating: In/3.jpg                \n",
            "  inflating: In/4.jpg                \n",
            "  inflating: In/5.jpg                \n",
            "  inflating: In/6.jpg                \n",
            "  inflating: In/7.jpg                \n",
            "  inflating: In/8.jpg                \n",
            "  inflating: In/9.jpg                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## easyOCR"
      ],
      "metadata": {
        "id": "TmZ39Q9GNGTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip install python-bidi==0.4.2\n",
        "#!pip install easyocr\n",
        "\n",
        "import easyocr\n",
        "def easyocr_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses EasyOCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    reader = easyocr.Reader(['en'])\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        image = cv2.imread(image_file)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        easyocr_results = reader.readtext(image_rgb)\n",
        "        easyocr_boxes = []\n",
        "        for result in easyocr_results:\n",
        "            box = result[0]\n",
        "            easyocr_boxes.append([int(min(point[0] for point in box)), int(min(point[1] for point in box)),\n",
        "                                  int(max(point[0] for point in box)), int(max(point[1] for point in box))])\n",
        "\n",
        "        output_image = draw_boxes(image_rgb.copy(), easyocr_boxes, color=(0, 255, 0))  # Green\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "easyocr_ocr('/content/In', '/content/easyOCR')\n"
      ],
      "metadata": {
        "id": "8StUv4BHQ9nO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tesseract_ocr library"
      ],
      "metadata": {
        "id": "b_8pb72fM82k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get install tesseract-ocr\n",
        "#!pip install pytesseract\n",
        "\n",
        "import pytesseract\n",
        "\n",
        "def tesseract_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses Tesseract OCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        image = cv2.imread(image_file)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        tesseract_data = pytesseract.image_to_boxes(image).splitlines()\n",
        "        tesseract_boxes = []\n",
        "        for line in tesseract_data:\n",
        "            parts = line.split(' ')\n",
        "            if len(parts) >= 6:\n",
        "                x1, y1, x2, y2 = map(int, parts[1:5])\n",
        "                tesseract_boxes.append((x1, image.shape[0] - y2, x2, image.shape[0] - y1))\n",
        "\n",
        "        output_image = draw_boxes(image_rgb.copy(), tesseract_boxes, color=(255, 0, 0))  # Red\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "tesseract_ocr('/content/In', 'pytesseract')\n"
      ],
      "metadata": {
        "id": "4I0tiqG8WIYd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PaddleOCR"
      ],
      "metadata": {
        "id": "s9OvfHLTMtLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 install paddlepaddle paddleocr\n",
        "from paddleocr import PaddleOCR\n",
        "\n",
        "def paddleocr_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses PaddleOCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        image = cv2.imread(image_file)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        paddleocr_results = ocr.ocr(image_file, cls=True)\n",
        "        paddleocr_boxes = []\n",
        "        for result in paddleocr_results:\n",
        "            for line in result:\n",
        "                box = line[0]\n",
        "                paddleocr_boxes.append([int(min(point[0] for point in box)), int(min(point[1] for point in box)),\n",
        "                                        int(max(point[0] for point in box)), int(max(point[1] for point in box))])\n",
        "\n",
        "        output_image = draw_boxes(image_rgb.copy(), paddleocr_boxes, color=(0, 0, 255))  # Blue\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "paddleocr_ocr('/content/In', 'PaddleOCR')\n"
      ],
      "metadata": {
        "id": "Ua-AhBzGXCLP",
        "outputId": "ecbdcc54-4b7d-40f8-971a-f0b726dc10d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.00M/4.00M [00:13<00:00, 291kiB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar to /root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/en_PP-OCRv4_rec_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.2M/10.2M [00:10<00:00, 992kiB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to /root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.19M/2.19M [00:12<00:00, 178kiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024/07/22 07:39:47] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/usr/local/lib/python3.10/dist-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024/07/22 07:39:48] ppocr DEBUG: dt_boxes num : 8, elapsed : 0.32001304626464844\n",
            "[2024/07/22 07:39:48] ppocr DEBUG: cls num  : 8, elapsed : 0.06385326385498047\n",
            "[2024/07/22 07:39:49] ppocr DEBUG: rec_res num  : 8, elapsed : 0.40788793563842773\n",
            "[2024/07/22 07:39:49] ppocr DEBUG: dt_boxes num : 28, elapsed : 0.12374424934387207\n",
            "[2024/07/22 07:39:49] ppocr DEBUG: cls num  : 28, elapsed : 0.07362890243530273\n",
            "[2024/07/22 07:39:50] ppocr DEBUG: rec_res num  : 28, elapsed : 1.0347530841827393\n",
            "[2024/07/22 07:39:50] ppocr DEBUG: dt_boxes num : 30, elapsed : 0.2462143898010254\n",
            "[2024/07/22 07:39:50] ppocr DEBUG: cls num  : 30, elapsed : 0.08752655982971191\n",
            "[2024/07/22 07:39:52] ppocr DEBUG: rec_res num  : 30, elapsed : 1.578695297241211\n",
            "[2024/07/22 07:39:53] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.42932844161987305\n",
            "[2024/07/22 07:39:53] ppocr DEBUG: cls num  : 6, elapsed : 0.022837162017822266\n",
            "[2024/07/22 07:39:54] ppocr DEBUG: rec_res num  : 6, elapsed : 1.2392323017120361\n",
            "[2024/07/22 07:39:54] ppocr DEBUG: dt_boxes num : 16, elapsed : 0.22235393524169922\n",
            "[2024/07/22 07:39:54] ppocr DEBUG: cls num  : 16, elapsed : 0.035309791564941406\n",
            "[2024/07/22 07:39:57] ppocr DEBUG: rec_res num  : 16, elapsed : 2.799480438232422\n",
            "[2024/07/22 07:39:58] ppocr DEBUG: dt_boxes num : 12, elapsed : 0.37453746795654297\n",
            "[2024/07/22 07:39:58] ppocr DEBUG: cls num  : 12, elapsed : 0.02671504020690918\n",
            "[2024/07/22 07:39:58] ppocr DEBUG: rec_res num  : 12, elapsed : 0.7940599918365479\n",
            "[2024/07/22 07:39:59] ppocr DEBUG: dt_boxes num : 19, elapsed : 0.39278221130371094\n",
            "[2024/07/22 07:39:59] ppocr DEBUG: cls num  : 19, elapsed : 0.07713007926940918\n",
            "[2024/07/22 07:40:07] ppocr DEBUG: rec_res num  : 19, elapsed : 7.918247222900391\n",
            "[2024/07/22 07:40:08] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.19569015502929688\n",
            "[2024/07/22 07:40:08] ppocr DEBUG: cls num  : 9, elapsed : 0.04831695556640625\n",
            "[2024/07/22 07:40:09] ppocr DEBUG: rec_res num  : 9, elapsed : 1.0472486019134521\n",
            "[2024/07/22 07:40:09] ppocr DEBUG: dt_boxes num : 16, elapsed : 0.2767331600189209\n",
            "[2024/07/22 07:40:09] ppocr DEBUG: cls num  : 16, elapsed : 0.043619632720947266\n",
            "[2024/07/22 07:40:11] ppocr DEBUG: rec_res num  : 16, elapsed : 1.5186808109283447\n",
            "[2024/07/22 07:40:11] ppocr DEBUG: dt_boxes num : 6, elapsed : 0.23723411560058594\n",
            "[2024/07/22 07:40:11] ppocr DEBUG: cls num  : 6, elapsed : 0.013963699340820312\n",
            "[2024/07/22 07:40:11] ppocr DEBUG: rec_res num  : 6, elapsed : 0.263913631439209\n",
            "[2024/07/22 07:40:12] ppocr DEBUG: dt_boxes num : 4, elapsed : 0.3129699230194092\n",
            "[2024/07/22 07:40:12] ppocr DEBUG: cls num  : 4, elapsed : 0.012369394302368164\n",
            "[2024/07/22 07:40:12] ppocr DEBUG: rec_res num  : 4, elapsed : 0.30134153366088867\n",
            "[2024/07/22 07:40:13] ppocr DEBUG: dt_boxes num : 9, elapsed : 0.3029351234436035\n",
            "[2024/07/22 07:40:13] ppocr DEBUG: cls num  : 9, elapsed : 0.03371930122375488\n",
            "[2024/07/22 07:40:13] ppocr DEBUG: rec_res num  : 9, elapsed : 0.46839141845703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## doctr library"
      ],
      "metadata": {
        "id": "Fji4aAr9MjYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor\n",
        "\n",
        "def read_images(input_folder):\n",
        "    \"\"\"\n",
        "    Reads image files from the specified input folder.\n",
        "    Args:\n",
        "    input_folder (str): Path to the folder containing the input images.\n",
        "    Returns:\n",
        "    list: A list of file paths for the images in the input folder.\n",
        "    \"\"\"\n",
        "    image_files = []\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Ensure we read only image files\n",
        "            image_files.append(os.path.join(input_folder, file))\n",
        "    return image_files\n",
        "\n",
        "def save_image(output_folder, filename, image):\n",
        "    \"\"\"\n",
        "    Saves the image to the specified output folder.\n",
        "    Args:\n",
        "    output_folder (str): Path to the folder where the image will be saved.\n",
        "    filename (str): The name of the file to save the image as.\n",
        "    image (numpy.ndarray): The image to save.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    output_path = os.path.join(output_folder, f\"{filename}.png\")\n",
        "    cv2.imwrite(output_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def perform_ocr_doctr(image_path):\n",
        "    \"\"\"\n",
        "    Performs OCR on the image using the doctr library.\n",
        "    Args:\n",
        "    image_path (str): Path to the image file.\n",
        "    Returns:\n",
        "    Document: The OCR result as a doctr Document object.\n",
        "    \"\"\"\n",
        "    model = ocr_predictor(pretrained=True)\n",
        "    doc = DocumentFile.from_images(image_path)\n",
        "    result = model(doc)\n",
        "    return result\n",
        "\n",
        "def draw_boxes(image_path, result):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes around the recognized text in the image.\n",
        "    Args:\n",
        "    image_path (str): Path to the image file.\n",
        "    result (Document): The OCR result as a doctr Document object.\n",
        "    Returns:\n",
        "    numpy.ndarray: The image with bounding boxes drawn around recognized text.\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "    for page in result.pages:\n",
        "        for block in page.blocks:\n",
        "            for line in block.lines:\n",
        "                for word in line.words:\n",
        "                    # Extract bounding box coordinates\n",
        "                    (x_min, y_min), (x_max, y_max) = word.geometry\n",
        "                    # Convert normalized coordinates to pixel coordinates\n",
        "                    box = [\n",
        "                        int(x_min * width), int(y_min * height),\n",
        "                        int(x_max * width), int(y_max * height)\n",
        "                    ]\n",
        "                    # Draw rectangle on the image\n",
        "                    cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "    return image\n",
        "\n",
        "def extract_text(result):\n",
        "    \"\"\"\n",
        "    Extracts text content from the OCR result.\n",
        "    Args:\n",
        "    result (Document): The OCR result as a doctr Document object.\n",
        "    Returns:\n",
        "    str: The extracted text content.\n",
        "    \"\"\"\n",
        "    text_content = \"\"\n",
        "    for page in result.pages:\n",
        "        for block in page.blocks:\n",
        "            for line in block.lines:\n",
        "                for word in line.words:\n",
        "                    text_content += word.value + \" \"\n",
        "                text_content += \"\\n\"\n",
        "            text_content += \"\\n\"\n",
        "        text_content += \"\\n\"\n",
        "    return text_content\n",
        "\n",
        "def apply_doctr_model(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Applies the doctr OCR model to images in the input folder and saves the results.\n",
        "    Args:\n",
        "    input_folder (str): Path to the folder containing the input images.\n",
        "    output_folder (str): Path to the folder where the results will be saved.\n",
        "    \"\"\"\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        result = perform_ocr_doctr(image_file)\n",
        "\n",
        "        # Draw bounding boxes on the image\n",
        "        annotated_image = draw_boxes(image_file, result)\n",
        "\n",
        "        # Save the annotated image\n",
        "        base_name = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, base_name, annotated_image)\n",
        "\n",
        "        # Extract and save text content to a text file\n",
        "        text_content = extract_text(result)\n",
        "        text_file = os.path.join(output_folder, f\"{base_name}.txt\")\n",
        "        with open(text_file, 'w') as f:\n",
        "            f.write(text_content)\n",
        "\n",
        "# Paths\n",
        "input_folder = '/content/In'\n",
        "output_folder = '/content/OCR_Doctr'\n",
        "\n",
        "# Apply the OCR model\n",
        "apply_doctr_model(input_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "UXwfX0ieMh5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "\n",
        "def preprocess_and_tesseract_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Preprocess images using OpenCV and uses Tesseract OCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        image = cv2.imread(image_file)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        edged = cv2.Canny(blur, 50, 150)\n",
        "\n",
        "        tesseract_data = pytesseract.image_to_boxes(edged).splitlines()\n",
        "        tesseract_boxes = []\n",
        "        for line in tesseract_data:\n",
        "            parts = line.split(' ')\n",
        "            if len(parts) >= 6:\n",
        "                x1, y1, x2, y2 = map(int, parts[1:5])\n",
        "                tesseract_boxes.append((x1, image.shape[0] - y2, x2, image.shape[0] - y1))\n",
        "\n",
        "        output_image = draw_boxes(cv2.cvtColor(edged, cv2.COLOR_GRAY2RGB), tesseract_boxes, color=(255, 0, 0))  # Red\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "preprocess_and_tesseract_ocr('/content/In', '/content/pytesseract_opncv')\n"
      ],
      "metadata": {
        "id": "LPMp8cMDOCnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## keras_ocr"
      ],
      "metadata": {
        "id": "W-wT3E8AOOsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keras_ocr\n",
        "\n",
        "import keras_ocr\n",
        "def keras_ocr_function(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses Keras-OCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        image = keras_ocr.tools.read(image_file)\n",
        "        predictions = pipeline.recognize([image])[0]\n",
        "        boxes = []\n",
        "        for prediction in predictions:\n",
        "            box = prediction[1]\n",
        "            x1, y1 = int(box[0][0]), int(box[0][1])\n",
        "            x2, y2 = int(box[2][0]), int(box[2][1])\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "        output_image = draw_boxes(image.copy(), boxes, color=(0, 255, 0))  # Green\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "keras_ocr_function('/content/In', 'Keras_ocr')\n"
      ],
      "metadata": {
        "id": "vhZ7WSEjONeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ‘‡ failure tries part of the codes ðŸ‘‡"
      ],
      "metadata": {
        "id": "qkRO1DVlQF74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google_vision_ocr -> non open source library (not working)"
      ],
      "metadata": {
        "id": "pQ4lYD8aNfZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import vision\n",
        "import io\n",
        "\n",
        "def google_vision_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses Google Cloud Vision OCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        with io.open(image_file, 'rb') as image_file_obj:\n",
        "            content = image_file_obj.read()\n",
        "        image = vision.Image(content=content)\n",
        "        response = client.text_detection(image=image)\n",
        "        texts = response.text_annotations\n",
        "        google_boxes = []\n",
        "        for text in texts:\n",
        "            vertices = text.bounding_poly.vertices\n",
        "            google_boxes.append([vertices[0].x, vertices[0].y, vertices[2].x, vertices[2].y])\n",
        "\n",
        "        image_cv2 = cv2.imread(image_file)\n",
        "        image_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
        "        output_image = draw_boxes(image_rgb.copy(), google_boxes, color=(0, 255, 255))  # Yellow\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "google_vision_ocr('/content/In', 'Google_Vision')\n"
      ],
      "metadata": {
        "id": "VzZCKaEnYoRc",
        "outputId": "a52054c9-f680-4021-e2f5-2ceae8c86f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'vision' from 'google.cloud' (unknown location)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0327fe92b65e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgoogle_vision_ocr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'vision' from 'google.cloud' (unknown location)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## aws_textract_ocr -> non open source library (not working)"
      ],
      "metadata": {
        "id": "nkInPInANvxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install boto3\n",
        "\n",
        "import boto3\n",
        "from botocore.exceptions import NoRegionError\n",
        "def aws_textract_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses AWS Textract OCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client = boto3.client('textract', region_name='your-region')  # specify your region\n",
        "        image_files = read_images(input_folder)\n",
        "        for image_file in image_files:\n",
        "            with open(image_file, 'rb') as document:\n",
        "                image_bytes = bytearray(document.read())\n",
        "            response = client.detect_document_text(Document={'Bytes': image_bytes})\n",
        "\n",
        "            aws_boxes = []\n",
        "            for item in response['Blocks']:\n",
        "                if item['BlockType'] == 'WORD':\n",
        "                    box = item['Geometry']['BoundingBox']\n",
        "                    width, height = Image.open(image_file).size\n",
        "                    x1 = int(box['Left'] * width)\n",
        "                    y1 = int(box['Top'] * height)\n",
        "                    x2 = int((box['Left'] + box['Width']) * width)\n",
        "                    y2 = int((box['Top'] + box['Height']) * height)\n",
        "                    aws_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "            image_cv2 = cv2.imread(image_file)\n",
        "            image_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
        "            output_image = draw_boxes(image_rgb.copy(), aws_boxes, color=(0, 128, 128))  # Teal\n",
        "            filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "            save_image(output_folder, filename, output_image)\n",
        "\n",
        "    except NoRegionError:\n",
        "        print(\"You must specify a region.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "aws_textract_ocr('/content/In', 'AWS_Textract')\n"
      ],
      "metadata": {
        "id": "EjQ5xCMcYuK7",
        "outputId": "a887050d-0810-4f79-880c-b15a56cdd427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: Unable to locate credentials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  azure_ocr -> non open source library (not working)"
      ],
      "metadata": {
        "id": "Z180k3fSN2_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import io\n",
        "\n",
        "def azure_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses Microsoft Azure OCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    subscription_key = \"your_subscription_key\"\n",
        "    endpoint = \"your_endpoint\"\n",
        "    client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        with io.open(image_file, 'rb') as image_file_obj:\n",
        "            image_data = image_file_obj.read()\n",
        "        response = client.read_in_stream(io.BytesIO(image_data), raw=True)\n",
        "        operation_location = response.headers['Operation-Location']\n",
        "        operation_id = operation_location.split('/')[-1]\n",
        "\n",
        "        result = client.get_read_result(operation_id)\n",
        "        azure_boxes = []\n",
        "        if result.status == 'succeeded':\n",
        "            for page in result.analyze_result.read_results:\n",
        "                for line in page.lines:\n",
        "                    bounding_box = line.bounding_box\n",
        "                    x1, y1, x2, y2, x3, y3, x4, y4 = bounding_box\n",
        "                    azure_boxes.append([int(x1), int(y1), int(x3), int(y3)])\n",
        "\n",
        "        image_cv2 = cv2.imread(image_file)\n",
        "        image_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
        "        output_image = draw_boxes(image_rgb.copy(), azure_boxes, color=(255, 128, 0))  # Orange\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "azure_ocr('/content/In', 'path/to/output_folder')\n"
      ],
      "metadata": {
        "id": "vc1Vs20FY4_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## calamari_ocr -> open source but is not working"
      ],
      "metadata": {
        "id": "mFSyA76YOWTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install calamari_ocr\n",
        "from calamari_ocr.ocr import Predictor\n",
        "import cv2\n",
        "\n",
        "def calamari_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses Calamari OCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    predictor = Predictor.load('/content/en-default.pyrnn')  # Replace with your model path\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        image = cv2.imread(image_file)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        result = predictor.predict(image_rgb)\n",
        "        boxes = []\n",
        "        for line in result:\n",
        "            box = line['bbox']\n",
        "            x1, y1 = int(box[0]), int(box[1])\n",
        "            x2, y2 = int(box[2]), int(box[3])\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "        output_image = draw_boxes(image_rgb.copy(), boxes, color=(255, 0, 255))  # Magenta\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "calamari_ocr('/content/In', 'CalamariOCR')\n"
      ],
      "metadata": {
        "id": "L5rxhhm_bj7x",
        "outputId": "572fb90c-480e-4477-dc9e-7ea6cf371db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'Predictor' has no attribute 'load'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-dbb2033c0b07>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mcalamari_ocr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/In'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CalamariOCR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-dbb2033c0b07>\u001b[0m in \u001b[0;36mcalamari_ocr\u001b[0;34m(input_folder, output_folder)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutput_folder\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0mto\u001b[0m \u001b[0msave\u001b[0m \u001b[0mhighlighted\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/en-default.pyrnn'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your model path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Predictor' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## kraken library -> open source but is not working"
      ],
      "metadata": {
        "id": "ZoGU5pPDP5VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kraken\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import kraken\n",
        "from kraken.lib import models, segmentation\n",
        "from kraken.lib.util import pil2im, im2pil\n",
        "from kraken.lib.vgsl import TorchVGSLModel\n",
        "\n",
        "def read_images(input_folder):\n",
        "    image_files = []\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_files.append(os.path.join(input_folder, file))\n",
        "    return image_files\n",
        "\n",
        "def save_image(output_folder, filename, image):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    output_path = os.path.join(output_folder, f\"{filename}.png\")\n",
        "    cv2.imwrite(output_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def perform_ocr(image_path, model_path):\n",
        "    image = Image.open(image_path).convert('L')\n",
        "    model = TorchVGSLModel.load_model(model_path)\n",
        "    line_regions = segmentation.extract_polygons(image, segmentation.segment(im=image, text_direction='horizontal-lr'))\n",
        "    ocr_text = \"\"\n",
        "    for line in line_regions:\n",
        "        ocr_text += model.predict_string(im2pil(line)) + \"\\n\"\n",
        "    return ocr_text\n",
        "\n",
        "def apply_ocropus_model(input_folder, output_folder, model_path):\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        # Perform OCR\n",
        "        ocr_text = perform_ocr(image_file, model_path)\n",
        "\n",
        "        # Save OCR result to a text file\n",
        "        base_name = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        text_file = os.path.join(output_folder, f\"{base_name}.txt\")\n",
        "        with open(text_file, 'w') as f:\n",
        "            f.write(ocr_text)\n",
        "\n",
        "        # Optionally, save highlighted images\n",
        "        image = cv2.imread(image_file)\n",
        "        highlighted_image = highlight_text_regions(image, ocr_text)\n",
        "        save_image(output_folder, base_name, highlighted_image)\n",
        "\n",
        "def highlight_text_regions(image, ocr_text):\n",
        "    # Implement your own logic to highlight text regions\n",
        "    # This is a placeholder function\n",
        "    return image\n",
        "\n",
        "# Paths\n",
        "input_folder = '/path/to/In'\n",
        "output_folder = '/path/to/OCRopus'\n",
        "model_path = '/path/to/model.mlmodel'\n",
        "\n",
        "# Apply the OCR model\n",
        "apply_ocropus_model(input_folder, output_folder, model_path)\n"
      ],
      "metadata": {
        "id": "tJuN0FaRbobI",
        "outputId": "3f26b7d7-ca13-4a61-83cc-d83d4ca473e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'kraken.serialization' has no attribute 'load_any'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-3634c53ea6c9>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Apply the OCR model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mapply_kraken_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-3634c53ea6c9>\u001b[0m in \u001b[0;36mapply_kraken_model\u001b[0;34m(input_folder, output_folder, model_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_kraken_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'kraken.serialization' has no attribute 'load_any'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo apt-get update\n",
        "#!sudo apt-get install gocr\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def gocr_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses GOCR to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        image = cv2.imread(image_file)\n",
        "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        temp_image_path = 'temp_gocr.pnm'\n",
        "        cv2.imwrite(temp_image_path, image_gray)\n",
        "\n",
        "        # Run GOCR\n",
        "        result = subprocess.run(['gocr', '-i', temp_image_path, '-a', '50'], stdout=subprocess.PIPE)\n",
        "        gocr_output = result.stdout.decode('utf-8')\n",
        "\n",
        "        # Parse GOCR output to get bounding boxes (GOCR does not provide bounding boxes natively)\n",
        "        gocr_boxes = []  # Implement a way to parse and convert text to bounding boxes if possible\n",
        "\n",
        "        output_image = draw_boxes(image.copy(), gocr_boxes, color=(0, 255, 0))  # Green\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "gocr_ocr('/content/In', 'gocr_ocr')\n"
      ],
      "metadata": {
        "id": "krVaR8eOoi2Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!sudo apt-get install ocrad\n",
        "\n",
        "def ocrad_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses Ocrad to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        temp_image_path = 'temp_ocrad.pnm'\n",
        "        image = cv2.imread(image_file)\n",
        "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        cv2.imwrite(temp_image_path, image_gray)\n",
        "\n",
        "        # Run Ocrad\n",
        "        result = subprocess.run(['ocrad', temp_image_path], stdout=subprocess.PIPE)\n",
        "        ocrad_output = result.stdout.decode('utf-8')\n",
        "\n",
        "        # Parse Ocrad output to get bounding boxes (Ocrad does not provide bounding boxes natively)\n",
        "        ocrad_boxes = []  # Implement a way to parse and convert text to bounding boxes if possible\n",
        "\n",
        "        output_image = draw_boxes(image.copy(), ocrad_boxes, color=(0, 255, 0))  # Green\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "ocrad_ocr('/content/In', 'ocrad_ocr')\n"
      ],
      "metadata": {
        "id": "om1mzQKImHq4",
        "outputId": "cb9498b0-557a-422d-e2be-29633e3f161d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xb1 in position 1: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a8e51b21ccf0>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mocrad_ocr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/In'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ocrad_ocr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-a8e51b21ccf0>\u001b[0m in \u001b[0;36mocrad_ocr\u001b[0;34m(input_folder, output_folder)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Run Ocrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ocrad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_image_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mocrad_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Parse Ocrad output to get bounding boxes (Ocrad does not provide bounding boxes natively)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb1 in position 1: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## kraken OCR -> open source but is not working"
      ],
      "metadata": {
        "id": "cgQtz735peYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kraken\n",
        "from PIL import Image\n",
        "import os\n",
        "input_folder = '/content/In'\n",
        "output_folder = '/content/kraken-ocr'\n",
        "from kraken import binarization, rpred, pageseg, serialization\n",
        "\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(('.png', '.jpg', '.jpeg', '.tiff')):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        # Binarize image\n",
        "        bin_img = binarization.nlbin(img)\n",
        "\n",
        "        # Segment the page to get bounding boxes\n",
        "        bounds = pageseg.segment(bin_img)\n",
        "\n",
        "        # Load the Kraken model\n",
        "        model = serialization.load_any('en-default')\n",
        "\n",
        "        # Perform OCR\n",
        "        ocr_records = rpred.rpred(network=model, im=bin_img, bounds=bounds)\n",
        "        ocr_text = '\\n'.join([r['text'] for r in ocr_records])\n",
        "\n",
        "        # Save OCR text to a file\n",
        "        txt_filename = os.path.splitext(filename)[0] + '.txt'\n",
        "        txt_path = os.path.join(output_folder, txt_filename)\n",
        "        with open(txt_path, 'w') as txt_file:\n",
        "            txt_file.write(ocr_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "5QwMfAAgpOUn",
        "outputId": "02adb895-5779-4a0f-c720-26472816344a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'kraken.serialization' has no attribute 'load_any'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-64ce62480ebc>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Load the Kraken model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Perform OCR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'kraken.serialization' has no attribute 'load_any'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cuneiform_ocr -> open source but its not working"
      ],
      "metadata": {
        "id": "vbPywniDPF6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo add-apt-repository ppa:alex-p/cuneiform\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get install cuneiform\n",
        "import subprocess\n",
        "\n",
        "def cuneiform_ocr(input_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Uses CuneiForm to read text from images in the input folder and save highlighted images in the output folder.\n",
        "\n",
        "    Args:\n",
        "    input_folder (str): Path to the input folder containing images.\n",
        "    output_folder (str): Path to the output folder to save highlighted images.\n",
        "    \"\"\"\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        temp_image_path = 'temp_cuneiform.bmp'\n",
        "        image = cv2.imread(image_file)\n",
        "        cv2.imwrite(temp_image_path, image)\n",
        "\n",
        "        # Run CuneiForm\n",
        "        subprocess.run(['cuneiform', '-f', 'hocr', '-o', 'temp_cuneiform.html', temp_image_path])\n",
        "\n",
        "        # Parse HOCR file to get bounding boxes\n",
        "        with open('temp_cuneiform.html', 'r') as file:\n",
        "            hocr_data = file.read()\n",
        "\n",
        "        cuneiform_boxes = []  # Implement a way to parse HOCR data to bounding boxes\n",
        "\n",
        "        output_image = draw_boxes(image.copy(), cuneiform_boxes, color=(0, 255, 0))  # Green\n",
        "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        save_image(output_folder, filename, output_image)\n",
        "\n",
        "cuneiform_ocr('/content/In', 'cuneiform_ocr')\n"
      ],
      "metadata": {
        "id": "kABG2lMqr6Wl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## microsoft TrOCR ->suaible for handwritten -> open souce -> its output is undefined   "
      ],
      "metadata": {
        "id": "Z_uu-SUHO2YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "\n",
        "def read_images(input_folder):\n",
        "    image_files = []\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_files.append(os.path.join(input_folder, file))\n",
        "    return image_files\n",
        "\n",
        "def perform_ocr_trocr(image_path):\n",
        "    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    generated_ids = model.generate(pixel_values)\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_text\n",
        "\n",
        "def apply_trocr_model(input_folder, output_folder):\n",
        "    image_files = read_images(input_folder)\n",
        "    for image_file in image_files:\n",
        "        ocr_text = perform_ocr_trocr(image_file)\n",
        "\n",
        "        # Save OCR result to a text file\n",
        "        base_name = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        text_file = os.path.join(output_folder, f\"{base_name}.txt\")\n",
        "\n",
        "        # Ensure output folder exists\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        with open(text_file, 'w') as f:\n",
        "            f.write(ocr_text)\n",
        "\n",
        "# Paths\n",
        "input_folder = '/content/In'\n",
        "output_folder = '/content/OCR_TroCR'\n",
        "\n",
        "# Apply the OCR model\n",
        "apply_trocr_model(input_folder, output_folder)"
      ],
      "metadata": {
        "id": "IgPcfvbRsTBb",
        "outputId": "c4a9de71-87aa-4ba9-8c3e-42eef7104b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}